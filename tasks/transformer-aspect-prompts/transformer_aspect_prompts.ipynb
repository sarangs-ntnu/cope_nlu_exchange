{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "pip install -q pandas scikit-learn numpy matplotlib seaborn torch torchvision torchaudio transformers datasets sentence-transformers mlflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Fine-Tuning with Aspect Prompts\n",
        "Fine-tune a single-head classifier that receives the aspect in the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv(Path('../../data/comments.csv'))\n",
        "df['prompt'] = 'Aspect: ' + df['aspect'] + ' | ' + df['comment']\n",
        "labels = sorted(df['label'].unique())\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "df['label_id'] = df['label'].map(label2id)\n",
        "ds = Dataset.from_pandas(df[['prompt','label_id']])\n",
        "ds = ds.train_test_split(test_size=0.2, seed=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['prompt'], truncation=True, padding='max_length', max_length=128)\n",
        "tokenized = ds.map(tokenize, batched=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased', num_labels=len(label2id), id2label={v:k for k,v in label2id.items()}, label2id=label2id\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='../../outputs/transformer-aspect-prompts',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=5e-5,\n",
        "    save_strategy='no'\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def collate(batch):\n",
        "    import torch\n",
        "    keys = ['input_ids','attention_mask']\n",
        "    out = {k: torch.tensor([b[k] for b in batch]) for k in keys}\n",
        "    out['labels'] = torch.tensor([b['label_id'] for b in batch])\n",
        "    return out\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['test'],\n",
        "    data_collator=collate,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}