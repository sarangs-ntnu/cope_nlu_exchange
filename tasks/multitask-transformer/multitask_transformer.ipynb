{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "pip install -q pandas scikit-learn numpy matplotlib seaborn torch torchvision torchaudio transformers datasets sentence-transformers mlflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multitask Transformer (Aspect + Sentiment)\n",
        "Fine-tune a shared encoder with two heads: one for aspect classification and one for sentiment classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv(Path('../../data/comments.csv'))\n",
        "df['prompt'] = 'Aspect: ' + df['aspect'] + ' | ' + df['comment']\n",
        "aspect_labels = sorted(df['aspect'].unique())\n",
        "sentiment_labels = sorted(df['label'].unique())\n",
        "aspect2id = {a:i for i,a in enumerate(aspect_labels)}\n",
        "sent2id = {s:i for i,s in enumerate(sentiment_labels)}\n",
        "df['aspect_id'] = df['aspect'].map(aspect2id)\n",
        "df['sent_id'] = df['label'].map(sent2id)\n",
        "ds = Dataset.from_pandas(df[['prompt','aspect_id','sent_id']])\n",
        "ds = ds.train_test_split(test_size=0.2, seed=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['prompt'], truncation=True, padding='max_length', max_length=128)\n",
        "tokenized = ds.map(tokenize, batched=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel, Trainer, TrainingArguments\n",
        "\n",
        "class DualHeadModel(nn.Module):\n",
        "    def __init__(self, base_model_name, num_aspects, num_sentiments):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "        self.aspect_head = nn.Linear(hidden, num_aspects)\n",
        "        self.sent_head = nn.Linear(hidden, num_sentiments)\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, aspect_labels=None, sent_labels=None):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.last_hidden_state[:,0]\n",
        "        aspect_logits = self.aspect_head(pooled)\n",
        "        sent_logits = self.sent_head(pooled)\n",
        "        loss = None\n",
        "        if aspect_labels is not None and sent_labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(aspect_logits, aspect_labels) + loss_fn(sent_logits, sent_labels)\n",
        "        return {'loss': loss, 'aspect_logits': aspect_logits, 'sent_logits': sent_logits}\n",
        "\n",
        "model = DualHeadModel('distilbert-base-uncased', len(aspect2id), len(sent2id))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def collate(batch):\n",
        "    keys = ['input_ids','attention_mask']\n",
        "    out = {k: torch.tensor([b[k] for b in batch]) for k in keys}\n",
        "    out['aspect_labels'] = torch.tensor([b['aspect_id'] for b in batch])\n",
        "    out['sent_labels'] = torch.tensor([b['sent_id'] for b in batch])\n",
        "    return out\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='../../outputs/multitask-transformer',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_steps=5,\n",
        "    learning_rate=5e-5,\n",
        "    save_strategy='no'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['test'],\n",
        "    data_collator=collate,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}